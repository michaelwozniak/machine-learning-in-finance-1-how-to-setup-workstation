# Machine Learning Development Environment Setup Guide

## üìö Table of Contents
1. [Introduction](#introduction)
2. [Prerequisites](#prerequisites)
3. [Installing Python](#installing-python)
4. [Installing Git](#installing-git)
5. [Command Line Fundamentals](#command-line-fundamentals)
6. [Installing uv (Python Package Manager)](#installing-uv-python-package-manager)
7. [Installing Visual Studio Code (IDE)](#installing-visual-studio-code-ide)
8. [Setting Up Your Development Environment](#setting-up-your-development-environment)
9. [Working with Python Scripts](#working-with-python-scripts)
10. [Working with Jupyter Notebooks](#working-with-jupyter-notebooks)
11. [Creating Your First ML Project](#creating-your-first-ml-project)
12. [Best Practices and Tips](#best-practices-and-tips)
13. [Troubleshooting](#troubleshooting)
14. [Google Colab: Cloud-Based Machine Learning Environment](#google-colab-cloud-based-machine-learning-environment)

## Introduction

Welcome to the world of Machine Learning! This comprehensive guide will help you set up a professional development environment on your computer. Whether you're using macOS or Windows, this tutorial will walk you through every step needed to start your journey in machine learning and data science.

### What You'll Learn
- How to install and configure Python, the primary language for ML
- How to use Git for version control and collaboration
- How to manage Python packages efficiently with uv
- How to set up Visual Studio Code as your development environment
- How to run Python scripts and Jupyter notebooks
- How to create and organize your first ML project

### Why These Tools?
- **Python**: The most popular language for machine learning and data science
- **Git**: Essential for version control and collaboration
- **uv**: Modern, fast Python package manager that replaces pip
- **VS Code**: Free, powerful IDE with excellent Python support

---

## Prerequisites

Before we start, make sure you have:
- A computer running macOS or Windows
- Administrator privileges on your computer
- Stable internet connection
- At least 5GB of free disk space

---

## Installing Python

Python is the foundation of our machine learning environment. We'll install Python 3.11 or later.

### macOS Installation

#### Option 1: Using Official Python Installer (Recommended for Beginners)

1. **Download Python**:
   - Visit [python.org](https://www.python.org/downloads/)
   - Click "Download Python 3.11.x" (or the latest version)
   - Download the macOS installer (.pkg file)

2. **Install Python**:
   - Double-click the downloaded .pkg file
   - Follow the installation wizard
   - Click "Install Now"

3. **Verify Installation**:
   ```bash
   # Open Terminal (Cmd + Space, type "Terminal")
   python3 --version
   # Should output: Python 3.11.x
   
   pip3 --version
   # Should output pip version
   ```

#### Option 2: Using Homebrew (Advanced Users)

```bash
# Install Homebrew first (if not installed)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Python
brew install python
```

### Windows Installation

1. **Download Python**:
   - Visit [python.org](https://www.python.org/downloads/)
   - Click "Download Python 3.11.x"
   - Download the Windows installer (.exe file)

2. **Install Python**:
   - Run the downloaded .exe file **as Administrator**
   - ‚ö†Ô∏è **IMPORTANT**: Check "Add Python to PATH" at the bottom
   - Click "Install Now"
   - If prompted, allow the installer to disable path length limit

3. **Verify Installation**:
   ```cmd
   # Open Command Prompt (Win + R, type "cmd")
   python --version
   # Should output: Python 3.11.x
   
   pip --version
   # Should output pip version
   ```

### Understanding Python

**What is Python?**
Python is a high-level, interpreted programming language known for its simplicity and readability. It's the go-to language for machine learning because:
- Simple syntax that's easy to learn
- Extensive libraries for data science (pandas, numpy, scikit-learn)
- Large community and excellent documentation
- Cross-platform compatibility

**Key Python Concepts**:
- **Interpreter**: Executes Python code line by line
- **Packages/Libraries**: Pre-written code you can use in your projects
- **Virtual Environments**: Isolated spaces for different projects
- **pip**: Python's package installer (we'll replace this with uv)

---

## Installing Git

Git is a version control system that tracks changes in your code and enables collaboration.

### macOS Installation

#### Option 1: Using Xcode Command Line Tools (Recommended)

```bash
# Install Xcode Command Line Tools
xcode-select --install
```

#### Option 2: Using Homebrew

```bash
brew install git
```

#### Option 3: Official Git Installer

1. Download from [git-scm.com](https://git-scm.com/download/mac)
2. Run the installer and follow instructions

### Windows Installation

1. **Download Git**:
   - Visit [git-scm.com](https://git-scm.com/download/win)
   - Download the 64-bit Git for Windows Setup

2. **Install Git**:
   - Run the installer as Administrator
   - **Important Settings**:
     - Select "Use Git from the command line and also from 3rd-party software"
     - Select "Checkout Windows-style, commit Unix-style line endings"
     - Select "Use Windows' default console window"

### Verify Git Installation

```bash
# macOS: Terminal / Windows: Command Prompt or Git Bash
git --version
# Should output: git version 2.x.x
```

### Setting Up Git

After installation, configure Git with your information:

```bash
# Set your name and email (use your real information)
git config --global user.name "Your Full Name"
git config --global user.email "your.email@example.com"

# Verify configuration
git config --list
```

### Understanding Git

**What is Git?**
Git is a distributed version control system that helps you:
- Track changes in your code over time
- Collaborate with others on projects
- Backup your work in remote repositories (like GitHub)
- Manage different versions of your project

**Key Git Concepts**:
- **Repository (repo)**: A project folder tracked by Git
- **Commit**: A snapshot of your project at a specific time
- **Branch**: A parallel version of your project
- **Remote**: A version of your repository hosted online (e.g., GitHub)
- **Clone**: Download a repository from a remote source
- **Push**: Upload your changes to a remote repository
- **Pull**: Download changes from a remote repository
- **Merge**: Combine changes from different branches
- **Pull Request**: Request to merge changes (GitHub feature)
- **Fork**: Create your own copy of someone else's repository

### Git Workflow Fundamentals

**The Three States of Git**:
1. **Working Directory**: Your current files (modified but not staged)
2. **Staging Area**: Files ready to be committed (git add)
3. **Repository**: Committed snapshots (git commit)

**Basic Git Workflow**:
```bash
# 1. Check status of your files
git status

# 2. Add files to staging area
git add filename.py          # Add specific file
git add .                    # Add all files
git add *.py                 # Add all Python files

# 3. Commit changes
git commit -m "Add new feature"

# 4. Push to remote repository
git push origin main
```

**Essential Git Commands**:
```bash
# Repository management
git init                     # Initialize new repository
git clone <url>              # Clone existing repository
git remote add origin <url>  # Add remote repository

# File management
git add <file>               # Stage files for commit
git rm <file>                # Remove file from tracking
git mv <old> <new>           # Rename file

# Commit management
git commit -m "message"      # Commit staged changes
git commit -am "message"     # Stage and commit all tracked files
git log                      # View commit history
git log --oneline            # Compact commit history

# Branch management
git branch                   # List branches
git branch <name>            # Create new branch
git checkout <branch>        # Switch to branch
git checkout -b <branch>     # Create and switch to branch
git merge <branch>           # Merge branch into current

# Remote operations
git push origin <branch>     # Push branch to remote
git pull origin <branch>     # Pull changes from remote
git fetch                    # Download changes without merging
```

---

## GitHub Setup and Integration

GitHub is a web-based platform that hosts Git repositories and provides collaboration tools.

### Creating a GitHub Account

1. **Visit GitHub**: Go to [github.com](https://github.com)
2. **Sign Up**: Click "Sign up" and create your account
3. **Choose Plan**: Free plan is sufficient for students
4. **Verify Email**: Check your email and verify your account

### Setting Up GitHub Authentication

#### Option 1: Personal Access Token (Recommended)

1. **Generate Token**:
   - Go to GitHub ‚Üí Settings ‚Üí Developer settings ‚Üí Personal access tokens ‚Üí Tokens (classic)
   - Click "Generate new token (classic)"
   - Give it a descriptive name (e.g., "My Development Machine")
   - Select scopes: `repo`, `workflow`, `write:packages`
   - Click "Generate token"
   - **Copy the token immediately** (you won't see it again!)

2. **Configure Git**:
   ```bash
   # Set up credential helper
   git config --global credential.helper store
   
   # Test authentication
   git clone https://github.com/yourusername/test-repo.git
   # When prompted:
   # Username: your-github-username
   # Password: paste-your-personal-access-token
   ```

#### Option 2: Command Line Setup (Advanced)

For users who prefer command-line setup or need to automate the process:

**Step 1: Generate Personal Access Token via Command Line**

```bash
# Install GitHub CLI (if not already installed)
# macOS
brew install gh

# Windows (using winget)
winget install GitHub.cli

# Linux
curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | sudo tee /etc/apt/sources.list.d/github-cli.list > /dev/null
sudo apt update
sudo apt install gh
```

**Step 2: Authenticate with GitHub CLI**

```bash
# Login to GitHub
gh auth login

# Follow the prompts:
# 1. Choose "GitHub.com"
# 2. Choose "HTTPS" as protocol
# 3. Choose "Yes" to authenticate Git with GitHub credentials
# 4. Choose "Login with a web browser"
# 5. Copy the one-time code and press Enter
# 6. Complete authentication in your browser
```

**Step 3: Configure Git Credentials**

```bash
# Set up credential helper for HTTPS
git config --global credential.helper store

# Set up credential helper for macOS Keychain (macOS only)
git config --global credential.helper osxkeychain

# Set up credential helper for Windows Credential Manager (Windows only)
git config --global credential.helper manager-core

# Verify configuration
git config --list | grep credential
```

**Step 4: Test Authentication**

```bash
# Test with a simple Git operation
git ls-remote https://github.com/octocat/Hello-World.git

# Or clone a test repository
git clone https://github.com/octocat/Hello-World.git
cd Hello-World
git remote -v
cd ..
rm -rf Hello-World
```

**Step 5: Set Up SSH Authentication (Alternative)**

If you prefer SSH over HTTPS:

```bash
# Generate SSH key
ssh-keygen -t ed25519 -C "your.email@example.com"

# Start SSH agent
eval "$(ssh-agent -s)"

# Add SSH key to agent
ssh-add ~/.ssh/id_ed25519

# Copy public key to clipboard
# macOS
pbcopy < ~/.ssh/id_ed25519.pub

# Windows (Git Bash)
clip < ~/.ssh/id_ed25519.pub

# Linux
cat ~/.ssh/id_ed25519.pub
```

Then add the SSH key to your GitHub account:

```bash
# Add SSH key to GitHub via CLI
gh ssh-key add ~/.ssh/id_ed25519.pub --title "My Development Machine"

# Or manually:
# 1. Go to GitHub ‚Üí Settings ‚Üí SSH and GPG keys
# 2. Click "New SSH key"
# 3. Paste your public key
# 4. Give it a descriptive title
```

**Step 6: Configure Git to Use SSH**

```bash
# Change remote URL to SSH
git remote set-url origin git@github.com:username/repository.git

# Or clone using SSH
git clone git@github.com:username/repository.git

# Test SSH connection
ssh -T git@github.com
```

**Step 7: Advanced Credential Management**

For multiple GitHub accounts or advanced scenarios:

```bash
# Create SSH config file
cat > ~/.ssh/config << 'EOF'
# Personal GitHub account
Host github.com-personal
    HostName github.com
    User git
    IdentityFile ~/.ssh/id_ed25519_personal

# Work GitHub account
Host github.com-work
    HostName github.com
    User git
    IdentityFile ~/.ssh/id_ed25519_work
EOF

# Use different hosts for different repositories
git clone git@github.com-personal:username/personal-repo.git
git clone git@github.com-work:company/work-repo.git
```

**Step 8: Environment Variables for Automation**

For CI/CD or automated scripts:

```bash
# Set environment variables (add to ~/.bashrc or ~/.zshrc)
export GITHUB_TOKEN="your_personal_access_token"
export GITHUB_USERNAME="your_username"

# Use in scripts
git clone https://$GITHUB_USERNAME:$GITHUB_TOKEN@github.com/username/repo.git

# Or use GitHub CLI
gh auth token
gh api user
```

**Step 9: Verify Complete Setup**

```bash
# Check Git configuration
git config --list

# Check GitHub CLI authentication
gh auth status

# Test Git operations
git ls-remote https://github.com/octocat/Hello-World.git

# Test GitHub CLI operations
gh repo list
gh issue list
```

**Troubleshooting Command Line Setup:**

```bash
# Clear stored credentials
git config --global --unset credential.helper
rm ~/.git-credentials

# Re-authenticate
gh auth logout
gh auth login

# Check SSH connection
ssh -vT git@github.com

# Reset GitHub CLI
gh auth logout
gh auth login --web
```


### Creating Your First Repository

#### Method 1: Using GitHub Website

1. **Create Repository**:
   - Go to GitHub.com
   - Click "+" ‚Üí "New repository"
   - Repository name: `my-first-ml-project`
   - Description: "My first machine learning project"
   - Choose Public or Private
   - ‚úÖ Check "Add a README file"
   - ‚úÖ Check "Add .gitignore" ‚Üí Python
   - Click "Create repository"

2. **Clone to Local Machine**:
   ```bash
   # Clone the repository
   git clone https://github.com/yourusername/my-first-ml-project.git
   cd my-first-ml-project
   ```

#### Method 2: From Local Project

1. **Initialize Local Repository**:
   ```bash
   # In your project directory
   git init
   git add .
   git commit -m "Initial commit"
   ```

2. **Connect to GitHub**:
   ```bash
   # Add remote origin
   git remote add origin https://github.com/yourusername/my-first-ml-project.git
   
   # Push to GitHub
   git branch -M main
   git push -u origin main
   ```

### GitHub Repository Structure

**Essential Files for ML Projects**:

1. **README.md**: Project description and instructions
2. **.gitignore**: Files to ignore in version control
3. **requirements.txt** or **pyproject.toml**: Dependencies
4. **LICENSE**: Open source license
5. **CONTRIBUTING.md**: How others can contribute
6. **docs/**: Documentation folder

**Sample README.md**:
```markdown
# My First ML Project

A machine learning project for predicting housing prices.

## üöÄ Quick Start

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/my-first-ml-project.git
   cd my-first-ml-project
   ```

2. Install dependencies:
   ```bash
   uv sync
   ```

3. Run the project:
   ```bash
   uv run jupyter notebook
   ```

## üìÅ Project Structure

```
my-first-ml-project/
‚îú‚îÄ‚îÄ data/           # Data files
‚îú‚îÄ‚îÄ notebooks/      # Jupyter notebooks
‚îú‚îÄ‚îÄ src/           # Source code
‚îú‚îÄ‚îÄ tests/         # Unit tests
‚îî‚îÄ‚îÄ README.md      # This file
```

## üß™ Features

- Data exploration and visualization
- Machine learning model training
- Model evaluation and comparison

## üìä Results

[Add your results and visualizations here]

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## üìÑ License

This project is licensed under the MIT License.
```

### Git Branching Strategy

**Why Use Branches?**
- Work on features without affecting main code
- Experiment safely
- Collaborate without conflicts
- Maintain stable main branch

**Common Branching Workflow**:

1. **Main Branch**: Stable, working code
2. **Feature Branches**: New features or experiments
3. **Bugfix Branches**: Fixing issues
4. **Development Branch**: Integration branch (optional)

**Branch Commands**:
```bash
# Create and switch to new branch
git checkout -b feature/new-analysis

# Work on your feature...
git add .
git commit -m "Add new analysis feature"

# Switch back to main
git checkout main

# Merge feature branch
git merge feature/new-analysis

# Delete feature branch
git branch -d feature/new-analysis
```

### Collaborative Workflow with GitHub

#### Working with Others

1. **Fork Repository**:
   - Go to someone else's repository
   - Click "Fork" button
   - Creates your own copy

2. **Clone Your Fork**:
   ```bash
   git clone https://github.com/yourusername/forked-repo.git
   cd forked-repo
   ```

3. **Add Original as Upstream**:
   ```bash
   git remote add upstream https://github.com/original-owner/repo.git
   ```

4. **Create Feature Branch**:
   ```bash
   git checkout -b feature/my-contribution
   ```

5. **Make Changes and Push**:
   ```bash
   git add .
   git commit -m "Add my contribution"
   git push origin feature/my-contribution
   ```

6. **Create Pull Request**:
   - Go to GitHub
   - Click "Compare & pull request"
   - Describe your changes
   - Submit PR

#### Managing Pull Requests

**As a Contributor**:
- Write clear commit messages
- Keep PRs focused and small
- Respond to feedback promptly
- Test your changes thoroughly

**As a Maintainer**:
- Review code carefully
- Provide constructive feedback
- Merge when ready
- Close PRs that won't be merged

### Advanced Git Features

#### Git Hooks

**Pre-commit Hook Example**:
```bash
# Create .git/hooks/pre-commit
#!/bin/sh
# Run tests before commit
python -m pytest tests/
if [ $? -ne 0 ]; then
    echo "Tests failed. Commit aborted."
    exit 1
fi
```

#### Git Submodules

**For Large Projects**:
```bash
# Add submodule
git submodule add https://github.com/user/repo.git external/repo

# Clone with submodules
git clone --recursive https://github.com/user/main-repo.git

# Update submodules
git submodule update --remote
```

#### Git Tags

**For Releases**:
```bash
# Create tag
git tag -a v1.0.0 -m "Release version 1.0.0"

# Push tags
git push origin v1.0.0

# List tags
git tag
```

### GitHub Features for ML Projects

#### GitHub Issues

**Use Issues for**:
- Bug reports
- Feature requests
- Questions and discussions
- Project planning

**Creating Good Issues**:
- Clear, descriptive title
- Detailed description
- Steps to reproduce (for bugs)
- Expected vs actual behavior
- Screenshots or code examples

#### GitHub Projects

**Project Management**:
- Kanban boards for task tracking
- Milestones for releases
- Labels for categorization
- Automation rules

#### GitHub Actions (CI/CD)

**Automated Workflows**:
```yaml
# .github/workflows/test.yml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.11
    - name: Install dependencies
      run: |
        pip install uv
        uv sync
    - name: Run tests
      run: uv run pytest
```

#### GitHub Pages

**Host Documentation**:
- Enable in repository settings
- Use Jekyll or custom HTML
- Perfect for project documentation
- Automatic deployment from main branch

### Best Practices for Git and GitHub

#### Commit Messages

**Good Commit Messages**:
```bash
# Format: <type>(<scope>): <description>
git commit -m "feat(data): add data preprocessing pipeline"
git commit -m "fix(model): correct accuracy calculation"
git commit -m "docs(readme): update installation instructions"
git commit -m "test(utils): add unit tests for data validation"
```

**Types**:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code formatting
- `refactor`: Code refactoring
- `test`: Adding tests
- `chore`: Maintenance tasks

#### Repository Management

**Keep Repositories Clean**:
- Use `.gitignore` effectively
- Don't commit large files (use Git LFS for data)
- Regular cleanup of branches
- Meaningful repository names

**Security Best Practices**:
- Never commit API keys or passwords
- Use environment variables
- Enable two-factor authentication
- Regular security updates

#### Collaboration Etiquette

**Code Reviews**:
- Be constructive and respectful
- Focus on code, not person
- Suggest improvements, not just problems
- Respond to feedback professionally

**Communication**:
- Use clear, descriptive commit messages
- Write good PR descriptions
- Respond to issues and PRs promptly
- Use GitHub's discussion features

---

## Command Line Fundamentals

The command line (also called terminal, shell, or command prompt) is a text-based interface for interacting with your computer. It's essential for machine learning development because most tools and workflows require command-line usage.

### What is the Command Line?

The command line is a text-based interface where you type commands to:
- Navigate your file system
- Run programs and scripts
- Manage files and directories
- Install software packages
- Control version control systems like Git
- Execute Python scripts and Jupyter notebooks

**Why Learn Command Line for ML?**
- Most ML tools are command-line based
- Git operations require terminal usage
- Package managers (pip, uv) work through command line
- Running Python scripts and notebooks
- Managing virtual environments
- Deploying models and applications

### Opening the Command Line

#### macOS
- **Terminal**: Applications ‚Üí Utilities ‚Üí Terminal
- **Spotlight**: Cmd + Space, type "Terminal"
- **Finder**: Go ‚Üí Utilities ‚Üí Terminal

#### Windows
- **Command Prompt**: Win + R, type "cmd"
- **PowerShell**: Win + X, select "Windows PowerShell"
- **Git Bash**: If you installed Git, use Git Bash for Unix-like commands

### Understanding the Command Line Interface

When you open the terminal, you'll see something like:

**macOS/Linux:**
```bash
username@computer-name:~$ 
```

**Windows Command Prompt:**
```cmd
C:\Users\username>
```

**Windows PowerShell:**
```powershell
PS C:\Users\username>
```

**Components:**
- **Prompt**: Shows your current location and user
- **Cursor**: Where you type commands
- **Working Directory**: Your current folder location

### Essential Command Line Concepts

#### Current Directory
Your **current directory** (or working directory) is the folder you're currently "in" when using the command line. Commands you run will affect files in this directory unless you specify otherwise.

#### Paths
A **path** tells the computer where to find a file or folder:
- **Absolute Path**: Complete location from root (e.g., `/Users/username/Documents/project`)
- **Relative Path**: Location relative to current directory (e.g., `../folder/file.txt`)

#### Commands
Commands are instructions you type to perform actions:
- **Command**: The action to perform (e.g., `ls`, `cd`, `mkdir`)
- **Arguments**: Additional information for the command (e.g., `ls -l`)
- **Options/Flags**: Modify command behavior (e.g., `-l` for detailed listing)

### Basic Navigation Commands

#### Listing Files and Directories

**macOS/Linux:**
```bash
# List files and folders in current directory
ls

# List with detailed information (permissions, size, date)
ls -l

# List all files including hidden ones
ls -a

# List with human-readable file sizes
ls -lh

# List files in a specific directory
ls /path/to/directory
```

**Windows Command Prompt:**
```cmd
# List files and folders
dir

# List with detailed information
dir /w

# List all files including hidden ones
dir /a

# List files in a specific directory
dir C:\path\to\directory
```

**Windows PowerShell:**
```powershell
# List files and folders
Get-ChildItem
# or
ls

# List with detailed information
Get-ChildItem -Force

# List files in a specific directory
Get-ChildItem C:\path\to\directory
```

#### Changing Directories

**macOS/Linux:**
```bash
# Change to home directory
cd ~
# or
cd

# Change to parent directory (go up one level)
cd ..

# Change to a specific directory
cd /path/to/directory

# Change to Documents folder
cd ~/Documents

# Go back to previous directory
cd -
```

**Windows Command Prompt:**
```cmd
# Change to a specific directory
cd C:\path\to\directory

# Change to Documents folder
cd %USERPROFILE%\Documents

# Change to parent directory
cd ..

# Go back to previous directory
cd -
```

**Windows PowerShell:**
```powershell
# Change to a specific directory
cd C:\path\to\directory

# Change to Documents folder
cd $env:USERPROFILE\Documents

# Change to parent directory
cd ..

# Go back to previous directory
cd -
```

#### Finding Your Current Location

**macOS/Linux:**
```bash
# Show current directory path
pwd
```

**Windows Command Prompt:**
```cmd
# Show current directory path
cd
```

**Windows PowerShell:**
```powershell
# Show current directory path
Get-Location
# or
pwd
```

### File and Directory Operations

#### Creating Directories

**macOS/Linux:**
```bash
# Create a single directory
mkdir new_folder

# Create multiple directories
mkdir folder1 folder2 folder3

# Create nested directories
mkdir -p path/to/nested/directory

# Create directory with spaces in name
mkdir "My Project Folder"
```

**Windows Command Prompt:**
```cmd
# Create a single directory
mkdir new_folder

# Create multiple directories
mkdir folder1 folder2 folder3

# Create nested directories
mkdir path\to\nested\directory
```

**Windows PowerShell:**
```powershell
# Create a single directory
New-Item -ItemType Directory -Name "new_folder"
# or
mkdir new_folder

# Create nested directories
New-Item -ItemType Directory -Path "path\to\nested\directory" -Force
```

#### Creating Files

**macOS/Linux:**
```bash
# Create an empty file
touch filename.txt

# Create multiple files
touch file1.txt file2.txt file3.txt

# Create file with content
echo "Hello World" > filename.txt

# Append content to file
echo "More content" >> filename.txt
```

**Windows Command Prompt:**
```cmd
# Create an empty file
type nul > filename.txt

# Create file with content
echo Hello World > filename.txt

# Append content to file
echo More content >> filename.txt
```

**Windows PowerShell:**
```powershell
# Create an empty file
New-Item -ItemType File -Name "filename.txt"

# Create file with content
"Hello World" | Out-File -FilePath "filename.txt"

# Append content to file
"More content" | Out-File -FilePath "filename.txt" -Append
```

#### Copying Files and Directories

**macOS/Linux:**
```bash
# Copy a file
cp source.txt destination.txt

# Copy file to another directory
cp file.txt /path/to/destination/

# Copy directory recursively
cp -r source_directory destination_directory

# Copy with progress
cp -v file.txt destination.txt
```

**Windows Command Prompt:**
```cmd
# Copy a file
copy source.txt destination.txt

# Copy directory recursively
xcopy source_directory destination_directory /E /I

# Copy with verification
copy /V source.txt destination.txt
```

**Windows PowerShell:**
```powershell
# Copy a file
Copy-Item source.txt destination.txt

# Copy directory recursively
Copy-Item source_directory destination_directory -Recurse

# Copy with progress
Copy-Item file.txt destination.txt -Verbose
```

#### Moving and Renaming Files

**macOS/Linux:**
```bash
# Move/rename a file
mv old_name.txt new_name.txt

# Move file to another directory
mv file.txt /path/to/destination/

# Move directory
mv old_directory new_directory
```

**Windows Command Prompt:**
```cmd
# Move/rename a file
move old_name.txt new_name.txt

# Move file to another directory
move file.txt C:\path\to\destination\

# Move directory
move old_directory new_directory
```

**Windows PowerShell:**
```powershell
# Move/rename a file
Move-Item old_name.txt new_name.txt

# Move file to another directory
Move-Item file.txt C:\path\to\destination\

# Move directory
Move-Item old_directory new_directory
```

#### Deleting Files and Directories

**macOS/Linux:**
```bash
# Delete a file
rm filename.txt

# Delete multiple files
rm file1.txt file2.txt file3.txt

# Delete directory and contents
rm -r directory_name

# Delete directory forcefully (no confirmation)
rm -rf directory_name

# Delete empty directory
rmdir empty_directory
```

**Windows Command Prompt:**
```cmd
# Delete a file
del filename.txt

# Delete directory and contents
rmdir /S directory_name

# Delete empty directory
rmdir directory_name
```

**Windows PowerShell:**
```powershell
# Delete a file
Remove-Item filename.txt

# Delete directory and contents
Remove-Item directory_name -Recurse -Force

# Delete empty directory
Remove-Item empty_directory
```

### Text Manipulation Commands

#### Viewing File Contents

**macOS/Linux:**
```bash
# Display entire file
cat filename.txt

# Display file with line numbers
cat -n filename.txt

# Display first 10 lines
head filename.txt

# Display last 10 lines
tail filename.txt

# Display specific number of lines
head -20 filename.txt
tail -20 filename.txt

# View file page by page
less filename.txt
# Use arrow keys to navigate, 'q' to quit
```

**Windows Command Prompt:**
```cmd
# Display entire file
type filename.txt

# Display first 10 lines
more filename.txt
```

**Windows PowerShell:**
```powershell
# Display entire file
Get-Content filename.txt

# Display first 10 lines
Get-Content filename.txt -Head 10

# Display last 10 lines
Get-Content filename.txt -Tail 10

# View file page by page
Get-Content filename.txt | Out-Host -Paging
```

#### Searching in Files

**macOS/Linux:**
```bash
# Search for text in a file
grep "search_term" filename.txt

# Search case-insensitive
grep -i "search_term" filename.txt

# Search in multiple files
grep "search_term" *.txt

# Search recursively in directories
grep -r "search_term" directory_name/

# Show line numbers
grep -n "search_term" filename.txt
```

**Windows Command Prompt:**
```cmd
# Search for text in a file
findstr "search_term" filename.txt

# Search case-insensitive
findstr /i "search_term" filename.txt

# Search in multiple files
findstr "search_term" *.txt
```

**Windows PowerShell:**
```powershell
# Search for text in a file
Select-String "search_term" filename.txt

# Search case-insensitive
Select-String -Pattern "search_term" -Path filename.txt -CaseSensitive:$false

# Search in multiple files
Select-String "search_term" *.txt
```

### Command Line Shortcuts and Tips

#### Keyboard Shortcuts

**Universal Shortcuts:**
- **Ctrl + C**: Cancel current command
- **Ctrl + D**: Exit terminal (or send EOF)
- **Ctrl + L**: Clear screen
- **Ctrl + R**: Search command history
- **Up/Down arrows**: Navigate command history
- **Tab**: Auto-complete file/directory names

**macOS/Linux Additional:**
- **Ctrl + A**: Move cursor to beginning of line
- **Ctrl + E**: Move cursor to end of line
- **Ctrl + U**: Delete from cursor to beginning of line
- **Ctrl + K**: Delete from cursor to end of line
- **Ctrl + W**: Delete previous word

#### Command History

**macOS/Linux:**
```bash
# View command history
history

# Run previous command
!!

# Run command from history by number
!123

# Search history interactively
Ctrl + R
```

**Windows Command Prompt:**
```cmd
# View command history
doskey /history

# Use arrow keys to navigate history
```

**Windows PowerShell:**
```powershell
# View command history
Get-History

# Run previous command
!!

# Run command from history by number
Invoke-History 123
```

#### Wildcards and Pattern Matching

**Universal Wildcards:**
- `*`: Matches any characters (e.g., `*.txt` matches all .txt files)
- `?`: Matches single character (e.g., `file?.txt` matches file1.txt, file2.txt)
- `[abc]`: Matches any character in brackets
- `[a-z]`: Matches any character in range

**Examples:**
```bash
# List all Python files
ls *.py

# List files starting with 'data'
ls data*

# List files with single character before .txt
ls ?.txt

# List files with numbers in name
ls file[0-9].txt
```

### Environment Variables

Environment variables store system-wide settings that programs can access.

#### Viewing Environment Variables

**macOS/Linux:**
```bash
# View all environment variables
env

# View specific variable
echo $HOME
echo $PATH

# View variable with more detail
printenv HOME
```

**Windows Command Prompt:**
```cmd
# View all environment variables
set

# View specific variable
echo %USERPROFILE%
echo %PATH%
```

**Windows PowerShell:**
```powershell
# View all environment variables
Get-ChildItem Env:

# View specific variable
$env:USERPROFILE
$env:PATH
```

#### Setting Environment Variables

**macOS/Linux:**
```bash
# Set variable for current session
export MY_VARIABLE="value"

# Set variable permanently (add to ~/.bashrc or ~/.zshrc)
echo 'export MY_VARIABLE="value"' >> ~/.bashrc
```

**Windows Command Prompt:**
```cmd
# Set variable for current session
set MY_VARIABLE=value

# Set variable permanently
setx MY_VARIABLE "value"
```

**Windows PowerShell:**
```powershell
# Set variable for current session
$env:MY_VARIABLE = "value"

# Set variable permanently
[Environment]::SetEnvironmentVariable("MY_VARIABLE", "value", "User")
```

### Command Line Tools for Machine Learning

#### Package Management

**Python Package Installation:**
```bash
# Install package with pip
pip install package_name

# Install specific version
pip install package_name==1.2.3

# Install from requirements file
pip install -r requirements.txt

# Install package in development mode
pip install -e .

# List installed packages
pip list

# Show package information
pip show package_name
```

**Using uv (Modern Package Manager):**
```bash
# Install package with uv
uv add package_name

# Install specific version
uv add "package_name==1.2.3"

# Install development dependencies
uv add --dev package_name

# Sync all dependencies
uv sync

# Run Python script with uv
uv run script.py

# Run Jupyter notebook
uv run jupyter notebook
```

#### Virtual Environment Management

**Creating Virtual Environments:**
```bash
# Create virtual environment with venv
python -m venv myenv

# Activate virtual environment (macOS/Linux)
source myenv/bin/activate

# Activate virtual environment (Windows)
myenv\Scripts\activate

# Deactivate virtual environment
deactivate
```

**Using uv for Virtual Environments:**
```bash
# Initialize new project with virtual environment
uv init my-project

# Install dependencies
uv add pandas numpy matplotlib

# Run commands in virtual environment
uv run python script.py
uv run jupyter notebook
```

#### Git Command Line Operations

**Basic Git Commands:**
```bash
# Initialize repository
git init

# Check status
git status

# Add files to staging
git add filename.py
git add .

# Commit changes
git commit -m "Add new feature"

# View commit history
git log

# Clone repository
git clone https://github.com/user/repo.git

# Push changes
git push origin main

# Pull changes
git pull origin main
```

#### File System Operations for ML Projects

**Organizing ML Project Structure:**
```bash
# Create project structure
mkdir -p my-ml-project/{data/{raw,processed},notebooks,src,tests,models,reports}

# Create empty files to preserve directory structure
touch data/raw/.gitkeep
touch data/processed/.gitkeep

# Copy data files
cp /path/to/dataset.csv data/raw/

# Move processed data
mv processed_data.csv data/processed/

# Create symbolic links for large datasets
ln -s /path/to/large/dataset data/external/
```

#### Process Management

**Running Background Processes:**
```bash
# Run command in background
python long_running_script.py &

# Run command with nohup (survives terminal closure)
nohup python training_script.py > training.log 2>&1 &

# View running processes
ps aux | grep python

# Kill process by ID
kill 1234

# Kill process by name
pkill python
```

**Monitoring System Resources:**
```bash
# Monitor CPU and memory usage
top

# Monitor disk usage
df -h

# Monitor directory size
du -sh directory_name/

# Monitor file changes
watch -n 1 ls -la
```

### Command Line Best Practices

#### File Naming Conventions
- Use lowercase letters and underscores
- Avoid spaces in filenames
- Use descriptive names
- Include version numbers when appropriate

**Good Examples:**
```bash
data_preprocessing.py
model_training_v2.py
housing_price_prediction.ipynb
requirements.txt
```

**Avoid:**
```bash
My Script.py
file1.txt
untitled notebook.ipynb
```

#### Command Organization
- Use clear, descriptive commands
- Chain commands with `&&` for sequential execution
- Use `|` (pipe) to pass output between commands
- Comment complex command sequences

**Examples:**
```bash
# Sequential execution
mkdir new_project && cd new_project && git init

# Pipe output
ls -la | grep ".py"

# Complex command with comments
# Create project structure and initialize git
mkdir -p {data,notebooks,src} && \
cd data && touch raw processed && \
cd .. && git init && git add .
```

#### Error Handling
- Always check command output for errors
- Use `&&` to stop on first error
- Redirect error output to files for debugging
- Test commands on small datasets first

**Examples:**
```bash
# Stop on first error
python preprocess.py && python train.py && python evaluate.py

# Capture errors
python script.py 2> error.log

# Check if command succeeded
if python script.py; then
    echo "Script completed successfully"
else
    echo "Script failed"
fi
```

### Common Command Line Patterns for ML

#### Data Processing Pipeline
```bash
# Download data
wget https://example.com/dataset.csv -O data/raw/dataset.csv

# Preprocess data
python src/preprocess.py data/raw/dataset.csv data/processed/clean_dataset.csv

# Train model
python src/train.py data/processed/clean_dataset.csv models/model.pkl

# Evaluate model
python src/evaluate.py models/model.pkl data/processed/test_data.csv
```

#### Batch Processing
```bash
# Process multiple files
for file in data/raw/*.csv; do
    python src/preprocess.py "$file" "data/processed/$(basename "$file")"
done

# Process with parallel execution
find data/raw -name "*.csv" | parallel python src/preprocess.py {} data/processed/{/}
```

#### Model Training and Evaluation
```bash
# Train multiple models
python src/train.py --model random_forest --data data/train.csv --output models/rf_model.pkl
python src/train.py --model gradient_boosting --data data/train.csv --output models/gb_model.pkl

# Cross-validation
python src/cross_validate.py --data data/train.csv --cv 5 --output results/cv_results.json

# Hyperparameter tuning
python src/tune_hyperparameters.py --data data/train.csv --output results/best_params.json
```

### Troubleshooting Command Line Issues

#### Common Problems and Solutions

**Problem: Command not found**
```bash
# Solution: Check if command is installed and in PATH
which command_name
echo $PATH

# Install missing command
# macOS: brew install command_name
# Ubuntu: sudo apt install command_name
# Windows: Use package manager or download installer
```

**Problem: Permission denied**
```bash
# Solution: Check file permissions
ls -la filename

# Change permissions
chmod +x script.py  # Make executable
chmod 644 filename  # Read/write for owner, read for others

# Run with sudo (use carefully)
sudo command_name
```

**Problem: File not found**
```bash
# Solution: Check current directory and file path
pwd
ls -la

# Use absolute path
/path/to/file

# Use relative path
../path/to/file
```

**Problem: Out of disk space**
```bash
# Check disk usage
df -h

# Find large files
find . -type f -size +100M

# Clean up temporary files
rm -rf /tmp/*
```

### Advanced Command Line Techniques

#### Scripting and Automation

**Creating Simple Scripts:**
```bash
# Create executable script
cat > train_model.sh << 'EOF'
#!/bin/bash
echo "Starting model training..."
python src/preprocess.py data/raw/train.csv data/processed/train_clean.csv
python src/train.py data/processed/train_clean.csv models/model.pkl
echo "Training completed!"
EOF

# Make script executable
chmod +x train_model.sh

# Run script
./train_model.sh
```

**Using Makefiles for ML Workflows:**
```makefile
# Makefile for ML project
.PHONY: all clean data train evaluate

all: data train evaluate

data:
	python src/download_data.py
	python src/preprocess.py

train:
	python src/train.py

evaluate:
	python src/evaluate.py

clean:
	rm -rf data/processed/*
	rm -rf models/*
```

#### Text Processing for Data Science

**CSV Manipulation:**
```bash
# View CSV file structure
head -5 data.csv

# Count lines
wc -l data.csv

# Extract specific columns
cut -d',' -f1,3,5 data.csv

# Filter rows
grep "pattern" data.csv

# Sort data
sort -t',' -k2 data.csv

# Remove duplicates
sort -u data.csv
```

**JSON Processing:**
```bash
# Pretty print JSON
cat data.json | python -m json.tool

# Extract specific fields
cat data.json | python -c "import json,sys; print(json.load(sys.stdin)['field'])"
```

#### System Monitoring for ML Workloads

**Monitor Training Progress:**
```bash
# Monitor GPU usage (if available)
nvidia-smi

# Monitor CPU and memory
htop

# Monitor disk I/O
iotop

# Monitor network usage
iftop
```

**Log Analysis:**
```bash
# View recent log entries
tail -f training.log

# Search for errors
grep -i error training.log

# Count occurrences
grep -c "epoch" training.log

# Extract specific time range
sed -n '/2024-01-01/,/2024-01-02/p' training.log
```

### Integration with Development Tools

#### VS Code Terminal Integration

**Opening Terminal in VS Code:**
- **Ctrl + `** (backtick): Open integrated terminal
- **Ctrl + Shift + `**: Create new terminal
- **Terminal ‚Üí New Terminal**: From menu

**Terminal Features:**
- Multiple terminal tabs
- Split terminal panes
- Terminal-specific settings
- Integrated with VS Code tasks

#### Jupyter Notebook Command Line

**Starting Jupyter:**
```bash
# Start Jupyter notebook
jupyter notebook

# Start Jupyter lab
jupyter lab

# Start with specific port
jupyter notebook --port 8889

# Start with specific directory
jupyter notebook --notebook-dir /path/to/project
```

**Jupyter Command Line Tools:**
```bash
# Convert notebook to Python script
jupyter nbconvert --to script notebook.ipynb

# Convert notebook to HTML
jupyter nbconvert --to html notebook.ipynb

# Execute notebook
jupyter nbconvert --execute notebook.ipynb
```

### Command Line Security Best Practices

#### Safe File Operations
- Always verify file paths before running destructive commands
- Use `-i` flag for interactive confirmation when available
- Test commands on copies of important files first
- Use version control for important projects

**Examples:**
```bash
# Interactive removal
rm -i filename.txt

# Safe copy with confirmation
cp -i source.txt destination.txt

# Create backup before modification
cp important_file.txt important_file.txt.backup
```

#### Secure Package Installation
- Verify package sources before installation
- Use virtual environments to isolate packages
- Regularly update packages for security patches
- Be cautious with `sudo` installations

**Examples:**
```bash
# Install in virtual environment (safer)
python -m venv myenv
source myenv/bin/activate
pip install package_name

# Check package before installation
pip show package_name
```

---

## Installing uv (Python Package Manager)

uv is a modern, extremely fast Python package manager that's much better than the traditional pip.

### Why uv instead of pip?

- **Speed**: 10-100x faster than pip
- **Better dependency resolution**: Avoids conflicts
- **Built-in virtual environment management**
- **Cross-platform compatibility**
- **Modern features**: lockfiles, workspace support

### Installation

#### macOS Installation

```bash
# Using curl (recommended)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Restart your terminal or run:
source ~/.bashrc  # or ~/.zshrc if using zsh
```

#### Windows Installation

```powershell
# Using PowerShell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

#### Alternative: Using pip (if above methods don't work)

```bash
pip install uv
# or on macOS:
pip3 install uv
```

### Verify uv Installation

```bash
uv --version
# Should output: uv 0.x.x
```

### Understanding uv

**Key uv Commands**:
- `uv init`: Create a new Python project
- `uv add package_name`: Add a package to your project
- `uv remove package_name`: Remove a package
- `uv run script.py`: Run a Python script in the project environment
- `uv sync`: Install all dependencies from lockfile
- `uv python install 3.11`: Install a specific Python version

**Benefits over pip**:
- Automatic virtual environment management
- Dependency resolution without conflicts
- Reproducible installations with lockfiles
- Much faster installation times

---

## Installing Visual Studio Code (IDE)

VS Code is a free, powerful code editor with excellent Python support.

### Installation

#### macOS Installation

1. **Download VS Code**:
   - Visit [code.visualstudio.com](https://code.visualstudio.com/)
   - Click "Download for Mac"
   - Download the .zip file

2. **Install VS Code**:
   - Unzip the downloaded file
   - Drag "Visual Studio Code" to your Applications folder
   - Launch from Applications or Spotlight

#### Windows Installation

1. **Download VS Code**:
   - Visit [code.visualstudio.com](https://code.visualstudio.com/)
   - Click "Download for Windows"
   - Download the .exe installer

2. **Install VS Code**:
   - Run the installer as Administrator
   - Accept the license agreement
   - ‚úÖ Check "Add to PATH" (important!)
   - ‚úÖ Check "Create a desktop icon"
   - Click "Install"

### Essential Extensions for Python/ML

After installing VS Code, install these essential extensions:

1. **Open VS Code**
2. **Open Extensions panel**: Ctrl+Shift+X (Windows) or Cmd+Shift+X (macOS)
3. **Search and install these extensions**:

   - **Python** (by Microsoft) - Essential for Python development
   - **Pylance** (by Microsoft) - Advanced Python language support
   - **Jupyter** (by Microsoft) - For Jupyter notebook support
   - **Python Debugger** (by Microsoft) - For debugging Python code
   - **Git Graph** (by mhutchie) - Visualize Git repository
   - **GitLens** (by GitKraken) - Enhanced Git capabilities
   - **Material Icon Theme** (by Philipp Kief) - Better file icons
   - **Bracket Pair Colorizer** (by CoenraadS) - Color matching brackets

### Configuring VS Code for Python

1. **Open Command Palette**: Ctrl+Shift+P (Windows) or Cmd+Shift+P (macOS)
2. **Type "Python: Select Interpreter"**
3. **Select your Python installation** (should show Python 3.11.x)

### Understanding IDEs and VS Code

**What is an IDE?**
An Integrated Development Environment (IDE) is a software application that provides comprehensive facilities for software development:
- Code editor with syntax highlighting
- Debugger for finding errors
- Build automation tools
- Version control integration

**VS Code Features for ML**:
- **IntelliSense**: Smart code completion
- **Debugging**: Step through code to find errors
- **Integrated Terminal**: Run commands without leaving the editor
- **Git Integration**: Manage version control visually
- **Extensions**: Add functionality for specific languages/frameworks
- **Jupyter Support**: Work with notebooks directly in the editor

---

## Setting Up Your Development Environment

Now let's create a proper development workspace and configure everything.

### Creating Your Development Folder

#### macOS
```bash
# Create a development folder in your home directory
mkdir -p ~/Development/machine-learning
cd ~/Development/machine-learning
```

#### Windows
```cmd
# Create a development folder
mkdir C:\Development\machine-learning
cd C:\Development\machine-learning
```

### Understanding Python Virtual Environments

**What are Virtual Environments?**
Virtual environments are isolated Python installations that allow you to:
- Install packages for specific projects without conflicts
- Use different Python versions for different projects
- Keep your system Python clean
- Share exact project requirements with others

**Why Use Virtual Environments?**
- **Isolation**: Each project has its own dependencies
- **Reproducibility**: Others can recreate your exact setup
- **Cleanliness**: Don't pollute your system Python installation
- **Flexibility**: Use different package versions for different projects

### Project Structure Best Practices

Let's create a standard project structure:

```
my-ml-project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/          # Original, immutable data
‚îÇ   ‚îú‚îÄ‚îÄ processed/    # Cleaned, processed data
‚îÇ   ‚îî‚îÄ‚îÄ external/     # Third-party data
‚îú‚îÄ‚îÄ notebooks/        # Jupyter notebooks for exploration
‚îú‚îÄ‚îÄ src/             # Source code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data/        # Data processing modules
‚îÇ   ‚îú‚îÄ‚îÄ models/      # ML model definitions
‚îÇ   ‚îî‚îÄ‚îÄ utils/       # Utility functions
‚îú‚îÄ‚îÄ tests/           # Unit tests
‚îú‚îÄ‚îÄ docs/            # Documentation
‚îú‚îÄ‚îÄ models/          # Trained model files
‚îú‚îÄ‚îÄ reports/         # Generated reports, figures
‚îú‚îÄ‚îÄ requirements.txt # Or pyproject.toml with uv
‚îú‚îÄ‚îÄ README.md        # Project description
‚îî‚îÄ‚îÄ .gitignore       # Files to ignore in version control
```

---

## Working with Python Scripts

Let's learn how to create and run Python scripts.

### Creating Your First Python Script

1. **Open VS Code**
2. **Create a new file**: File ‚Üí New File
3. **Save as**: `hello_ml.py`
4. **Add this code**:

```python
# hello_ml.py - Your first ML script
import sys
import platform

def main():
    print("üêç Welcome to Machine Learning!")
    print(f"Python version: {sys.version}")
    print(f"Platform: {platform.system()}")
    
    # Simple data analysis example
    numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    print(f"\nData: {numbers}")
    print(f"Sum: {sum(numbers)}")
    print(f"Average: {sum(numbers) / len(numbers)}")
    print(f"Max: {max(numbers)}")
    print(f"Min: {min(numbers)}")

if __name__ == "__main__":
    main()
```

### Running Python Scripts

#### Method 1: Using VS Code

1. **Right-click in the editor**
2. **Select "Run Python File in Terminal"**
3. **Or use the play button** in the top-right corner

#### Method 2: Using Terminal/Command Prompt

```bash
# Navigate to your script location
cd ~/Development/machine-learning  # macOS
# or
cd C:\Development\machine-learning  # Windows

# Run the script
python hello_ml.py          # Windows
python3 hello_ml.py         # macOS
```

#### Method 3: Using uv (Recommended for projects)

```bash
# If you have a uv project
uv run hello_ml.py
```

### Understanding Python Script Structure

**Key Components**:
- **Imports**: Bring in external libraries
- **Functions**: Reusable blocks of code
- **Main block**: `if __name__ == "__main__":` ensures code runs only when script is executed directly
- **Comments**: Explain what your code does

**Best Practices**:
- Use descriptive variable names
- Add comments to explain complex logic
- Structure code into functions
- Handle errors gracefully
- Follow PEP 8 style guidelines

---

## Working with Jupyter Notebooks

Jupyter notebooks are interactive documents that combine code, text, and visualizations.

### What are Jupyter Notebooks?

Jupyter notebooks are web-based interactive computing environments that allow you to:
- Write and execute code in cells
- Add explanatory text using Markdown
- Include visualizations and plots
- Share your analysis with others
- Prototype and experiment quickly

**When to Use Notebooks vs Scripts**:
- **Notebooks**: Data exploration, prototyping, tutorials, reports
- **Scripts**: Production code, automation, modules, testing

### Installing Jupyter

```bash
# Using uv (recommended)
uv add jupyter

# Or using pip
pip install jupyter        # Windows
pip3 install jupyter       # macOS
```

### Starting Jupyter Notebook

#### Method 1: From Terminal

```bash
# Navigate to your project folder
cd ~/Development/machine-learning

# Start Jupyter
jupyter notebook
```

This will open Jupyter in your web browser at `http://localhost:8888`

#### Method 2: Using VS Code

1. **Install Jupyter extension** (if not already installed)
2. **Create new file** with `.ipynb` extension
3. **VS Code will open it as a notebook**

### Creating Your First Notebook

1. **Create a new notebook**: New ‚Üí Python 3
2. **Rename**: Click "Untitled" and rename to "my_first_notebook"
3. **Add this content**:

**Cell 1 (Markdown)**:
```markdown
# My First Machine Learning Notebook

This notebook demonstrates basic Python concepts for machine learning.

## Objectives
- Learn notebook basics
- Explore data with Python
- Create simple visualizations
```

**Cell 2 (Code)**:
```python
# Import essential libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("Libraries imported successfully!")
```

**Cell 3 (Code)**:
```python
# Create sample data
np.random.seed(42)  # For reproducible results
data = {
    'student_id': range(1, 101),
    'study_hours': np.random.normal(5, 2, 100),
    'exam_score': np.random.normal(75, 15, 100)
}

df = pd.DataFrame(data)
print("Dataset created!")
print(f"Shape: {df.shape}")
df.head()
```

**Cell 4 (Code)**:
```python
# Basic statistics
print("Dataset Statistics:")
print(df.describe())
```

**Cell 5 (Code)**:
```python
# Create a simple visualization
plt.figure(figsize=(10, 6))
plt.scatter(df['study_hours'], df['exam_score'], alpha=0.6)
plt.xlabel('Study Hours')
plt.ylabel('Exam Score')
plt.title('Study Hours vs Exam Score')
plt.grid(True, alpha=0.3)
plt.show()
```

### Notebook Best Practices

1. **Use Descriptive Names**: Name notebooks clearly
2. **Add Markdown Cells**: Explain your analysis
3. **Keep Cells Short**: One concept per cell
4. **Clear Output**: Clear unnecessary output before saving
5. **Restart and Run All**: Test that your notebook runs from top to bottom
6. **Version Control**: Use `.ipynb_checkpoints/` in `.gitignore`

### Notebook Shortcuts

**Essential Keyboard Shortcuts**:
- `Shift + Enter`: Run cell and move to next
- `Ctrl + Enter`: Run cell and stay
- `A`: Insert cell above
- `B`: Insert cell below
- `DD`: Delete cell
- `M`: Change cell to Markdown
- `Y`: Change cell to Code
- `Esc`: Command mode
- `Enter`: Edit mode

---

## Creating Your First ML Project

Let's create a complete machine learning project from scratch.

### Setting Up the Project

```bash
# Create project directory
mkdir my-first-ml-project
cd my-first-ml-project

# Initialize uv project
uv init

# Add essential ML packages
uv add pandas numpy matplotlib seaborn scikit-learn jupyter
```

This creates:
- `pyproject.toml`: Project configuration
- `uv.lock`: Exact dependency versions
- `src/`: Source code directory
- `README.md`: Project description

### Project Structure

Create this folder structure:

```bash
# Create directories
mkdir -p data/{raw,processed}
mkdir -p notebooks
mkdir -p src/{data,models,utils}
mkdir -p tests
mkdir -p models
mkdir -p reports

# Create essential files
touch src/__init__.py
touch src/data/__init__.py
touch src/models/__init__.py
touch src/utils/__init__.py
touch .gitignore
```

### Setting up .gitignore

Create a `.gitignore` file to exclude files from version control:

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Jupyter Notebook
.ipynb_checkpoints

# Environment variables
.env

# Data files (optional - depends on your needs)
data/raw/*
!data/raw/.gitkeep
data/processed/*
!data/processed/.gitkeep

# Models (large files)
models/*.pkl
models/*.joblib
models/*.h5

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db
```

### Creating Sample Data

Create `src/data/generate_sample_data.py`:

```python
"""
Generate sample data for our first ML project
"""
import pandas as pd
import numpy as np
from pathlib import Path

def generate_housing_data(n_samples=1000):
    """Generate synthetic housing price data"""
    np.random.seed(42)
    
    # Generate features
    size = np.random.normal(1500, 500, n_samples)
    size = np.clip(size, 500, 5000)  # Realistic range
    
    bedrooms = np.random.poisson(2.5, n_samples) + 1
    bedrooms = np.clip(bedrooms, 1, 6)
    
    age = np.random.exponential(10, n_samples)
    age = np.clip(age, 0, 100)
    
    # Generate price based on features with some noise
    price = (
        size * 150 +  # $150 per sqft
        bedrooms * 5000 +  # $5000 per bedroom
        (50 - age) * 1000 +  # Newer houses worth more
        np.random.normal(0, 20000, n_samples)  # Random noise
    )
    price = np.clip(price, 50000, 1000000)  # Realistic range
    
    # Create DataFrame
    data = pd.DataFrame({
        'size_sqft': size.astype(int),
        'bedrooms': bedrooms.astype(int),
        'age_years': age.round(1),
        'price': price.astype(int)
    })
    
    return data

def main():
    """Generate and save sample data"""
    # Create data directory if it doesn't exist
    data_dir = Path('data/raw')
    data_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate data
    housing_data = generate_housing_data(1000)
    
    # Save to CSV
    output_path = data_dir / 'housing_data.csv'
    housing_data.to_csv(output_path, index=False)
    
    print(f"Generated {len(housing_data)} samples")
    print(f"Saved to: {output_path}")
    print("\nData preview:")
    print(housing_data.head())
    print("\nData info:")
    print(housing_data.describe())

if __name__ == "__main__":
    main()
```

### Running the Project

```bash
# Generate sample data
uv run src/data/generate_sample_data.py

# Start Jupyter notebook
uv run jupyter notebook
```

### Creating Analysis Notebook

Create `notebooks/01_exploratory_analysis.ipynb`:

```python
# Cell 1: Imports and setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Set style for better plots
plt.style.use('default')
sns.set_palette("husl")

print("Libraries loaded successfully!")

# Cell 2: Load data
data_path = Path('../data/raw/housing_data.csv')
df = pd.read_csv(data_path)

print(f"Dataset shape: {df.shape}")
print(f"Columns: {list(df.columns)}")
df.head()

# Cell 3: Basic statistics
print("Dataset Statistics:")
print(df.describe())

print("\nMissing values:")
print(df.isnull().sum())

# Cell 4: Visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Distribution of prices
axes[0, 0].hist(df['price'], bins=30, alpha=0.7)
axes[0, 0].set_title('Distribution of House Prices')
axes[0, 0].set_xlabel('Price ($)')

# Price vs Size
axes[0, 1].scatter(df['size_sqft'], df['price'], alpha=0.6)
axes[0, 1].set_title('Price vs Size')
axes[0, 1].set_xlabel('Size (sqft)')
axes[0, 1].set_ylabel('Price ($)')

# Price vs Bedrooms
df.boxplot(column='price', by='bedrooms', ax=axes[1, 0])
axes[1, 0].set_title('Price by Number of Bedrooms')

# Price vs Age
axes[1, 1].scatter(df['age_years'], df['price'], alpha=0.6)
axes[1, 1].set_title('Price vs Age')
axes[1, 1].set_xlabel('Age (years)')
axes[1, 1].set_ylabel('Price ($)')

plt.tight_layout()
plt.show()

# Cell 5: Correlation analysis
correlation_matrix = df.corr()
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
plt.show()
```

### Complete Project Setup with Git and GitHub

#### Step 1: Initialize Local Git Repository

```bash
# Navigate to your project directory
cd my-first-ml-project

# Initialize Git repository
git init

# Create initial .gitignore file
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Jupyter Notebook
.ipynb_checkpoints

# Environment variables
.env

# Data files (adjust based on your needs)
data/raw/*
!data/raw/.gitkeep
data/processed/*
!data/processed/.gitkeep

# Models (large files)
models/*.pkl
models/*.joblib
models/*.h5

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db

# uv
.venv/
uv.lock
EOF

# Create placeholder files for empty directories
touch data/raw/.gitkeep
touch data/processed/.gitkeep

# Add all files to Git
git add .

# Make initial commit
git commit -m "feat: initial project setup with ML structure"
```

#### Step 2: Create GitHub Repository

1. **Go to GitHub.com** and sign in
2. **Click "+" ‚Üí "New repository"**
3. **Repository settings**:
   - Name: `my-first-ml-project`
   - Description: "My first machine learning project - housing price prediction"
   - Visibility: Public (for portfolio) or Private
   - ‚ùå Don't initialize with README (we'll add our own)
   - ‚ùå Don't add .gitignore (we created our own)
   - ‚ùå Don't add license yet

4. **Click "Create repository"**

#### Step 3: Connect Local Repository to GitHub

```bash
# Add GitHub as remote origin
git remote add origin https://github.com/yourusername/my-first-ml-project.git

# Rename default branch to main
git branch -M main

# Push your code to GitHub
git push -u origin main
```

#### Step 4: Create Professional README

```bash
# Create comprehensive README.md
cat > README.md << 'EOF'
# üè† Housing Price Prediction Project

A machine learning project that predicts housing prices based on size, bedrooms, and age.

## üìä Project Overview

This project demonstrates fundamental machine learning concepts including:
- Data exploration and visualization
- Feature engineering
- Model training and evaluation
- Results interpretation

## üöÄ Quick Start

### Prerequisites
- Python 3.11+
- Git
- uv (Python package manager)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/my-first-ml-project.git
   cd my-first-ml-project
   ```

2. **Install dependencies**:
   ```bash
   uv sync
   ```

3. **Generate sample data**:
   ```bash
   uv run src/data/generate_sample_data.py
   ```

4. **Start Jupyter notebook**:
   ```bash
   uv run jupyter notebook
   ```

5. **Open `notebooks/01_exploratory_analysis.ipynb`** and run all cells

## üìÅ Project Structure

```
my-first-ml-project/
‚îú‚îÄ‚îÄ data/                    # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Original data
‚îÇ   ‚îî‚îÄ‚îÄ processed/          # Cleaned data
‚îú‚îÄ‚îÄ notebooks/              # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ 01_exploratory_analysis.ipynb
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ data/               # Data processing
‚îÇ   ‚îú‚îÄ‚îÄ models/             # ML models
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utilities
‚îú‚îÄ‚îÄ tests/                  # Unit tests
‚îú‚îÄ‚îÄ models/                 # Trained models
‚îú‚îÄ‚îÄ reports/                # Generated reports
‚îú‚îÄ‚îÄ pyproject.toml         # Project configuration
‚îú‚îÄ‚îÄ uv.lock                # Dependency lockfile
‚îî‚îÄ‚îÄ README.md               # This file
```

## üß™ Features

- **Data Generation**: Synthetic housing data with realistic patterns
- **Exploratory Analysis**: Comprehensive data exploration with visualizations
- **Statistical Analysis**: Correlation analysis and descriptive statistics
- **Visualization**: Interactive plots using matplotlib and seaborn

## üìà Results

[Add your analysis results and visualizations here]

### Key Findings
- Strong correlation between house size and price
- Bedroom count affects price significantly
- Older houses tend to be less expensive

## üîß Development

### Adding New Features

1. **Create feature branch**:
   ```bash
   git checkout -b feature/new-analysis
   ```

2. **Make your changes**

3. **Commit changes**:
   ```bash
   git add .
   git commit -m "feat: add new analysis feature"
   ```

4. **Push and create PR**:
   ```bash
   git push origin feature/new-analysis
   ```

### Running Tests

```bash
# Run all tests
uv run pytest

# Run specific test file
uv run pytest tests/test_data_processing.py
```

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìö Learning Resources

- [Python Documentation](https://docs.python.org/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Documentation](https://matplotlib.org/stable/)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë®‚Äçüíª Author

**Your Name**
- GitHub: [@yourusername](https://github.com/yourusername)
- Email: your.email@example.com

## üôè Acknowledgments

- Thanks to the open-source community for amazing tools
- Special thanks to course instructors for guidance
EOF

# Commit README
git add README.md
git commit -m "docs: add comprehensive README with project overview"
```

#### Step 5: Set Up Branch Protection and Workflow

```bash
# Create development branch for ongoing work
git checkout -b develop

# Push development branch
git push -u origin develop

# Switch back to main
git checkout main
```

#### Step 6: Create Your First Feature Branch

```bash
# Create feature branch for data analysis
git checkout -b feature/enhanced-analysis

# Make some changes (add new analysis)
echo "# Enhanced Analysis Features" >> notebooks/02_advanced_analysis.md

# Stage and commit changes
git add .
git commit -m "feat: add enhanced analysis documentation"

# Push feature branch
git push -u origin feature/enhanced-analysis
```

#### Step 7: Create Pull Request

1. **Go to GitHub repository**
2. **Click "Compare & pull request"** (appears after pushing feature branch)
3. **Fill out PR template**:
   - Title: "Add enhanced analysis documentation"
   - Description: "This PR adds documentation for enhanced analysis features"
   - Assign yourself as reviewer
   - Add labels: `enhancement`, `documentation`

4. **Submit the pull request**

#### Step 8: Merge and Clean Up

```bash
# After PR is approved and merged on GitHub, clean up locally
git checkout main
git pull origin main
git branch -d feature/enhanced-analysis
```

### Advanced Git Workflow for ML Projects

#### Working with Large Files (Data)

```bash
# Install Git LFS for large files
# macOS
brew install git-lfs

# Windows
# Download from https://git-lfs.github.io/

# Initialize Git LFS
git lfs install

# Track large files
git lfs track "*.csv"
git lfs track "*.pkl"
git lfs track "*.h5"

# Add .gitattributes file
git add .gitattributes
git commit -m "feat: add Git LFS tracking for large files"
```

#### Automated Workflows with GitHub Actions

Create `.github/workflows/ml-pipeline.yml`:

```yaml
name: ML Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        echo "$HOME/.cargo/bin" >> $GITHUB_PATH
    
    - name: Install dependencies
      run: uv sync
    
    - name: Run tests
      run: uv run pytest
    
    - name: Run data generation
      run: uv run src/data/generate_sample_data.py
    
    - name: Check notebook execution
      run: |
        uv add nbconvert
        uv run jupyter nbconvert --to notebook --execute notebooks/01_exploratory_analysis.ipynb
```

#### Project Management with GitHub

1. **Create Issues for Tasks**:
   - Go to Issues tab
   - Click "New issue"
   - Use templates for bug reports and feature requests

2. **Use Projects for Organization**:
   - Go to Projects tab
   - Create new project
   - Add issues to project board
   - Track progress with Kanban board

3. **Set Up Milestones**:
   - Go to Issues ‚Üí Milestones
   - Create milestones for project phases
   - Assign issues to milestones

### Best Practices for ML Project Management

#### Commit Message Convention

```bash
# Use conventional commits
git commit -m "feat(data): add data preprocessing pipeline"
git commit -m "fix(model): correct accuracy calculation bug"
git commit -m "docs(readme): update installation instructions"
git commit -m "test(utils): add unit tests for data validation"
git commit -m "refactor(notebooks): reorganize analysis workflow"
```

#### Branch Naming Convention

```bash
# Feature branches
feature/data-preprocessing
feature/model-training
feature/visualization

# Bug fix branches
bugfix/accuracy-calculation
bugfix/data-loading

# Hotfix branches
hotfix/critical-bug

# Release branches
release/v1.0.0
```

#### File Organization

```bash
# Keep data files organized
data/
‚îú‚îÄ‚îÄ raw/           # Original, immutable data
‚îú‚îÄ‚îÄ processed/     # Cleaned, processed data
‚îú‚îÄ‚îÄ external/      # Third-party data
‚îî‚îÄ‚îÄ interim/       # Intermediate data

# Document everything
docs/
‚îú‚îÄ‚îÄ data_dictionary.md
‚îú‚îÄ‚îÄ model_documentation.md
‚îú‚îÄ‚îÄ api_reference.md
‚îî‚îÄ‚îÄ deployment_guide.md
```

This comprehensive Git and GitHub setup provides students with professional-grade project management skills essential for modern software development and collaboration!

---

## Best Practices and Tips

### Code Organization

1. **Use Virtual Environments**: Always use uv or venv for projects
2. **Follow PEP 8**: Python style guidelines
3. **Write Documentation**: Comment your code and write README files
4. **Use Type Hints**: Makes code more readable and catches errors
5. **Write Tests**: Even simple tests are better than none

### Project Management

1. **Version Control Everything**: Use Git from day one
2. **Meaningful Commit Messages**: Describe what and why
3. **Regular Commits**: Small, frequent commits are better
4. **Backup Your Work**: Use GitHub, GitLab, or similar services

### Data Science Workflow

1. **Start with Exploration**: Use notebooks to understand your data
2. **Document Your Process**: Write down your findings and decisions
3. **Reproducible Results**: Set random seeds, save exact package versions
4. **Validate Your Models**: Always test on unseen data

### Performance Tips

1. **Use uv**: Much faster than pip for package management
2. **Profile Your Code**: Find bottlenecks before optimizing
3. **Vectorize Operations**: Use NumPy/Pandas operations instead of loops
4. **Use Appropriate Data Types**: Can save memory and improve speed

### Learning Resources

1. **Official Documentation**: Always start here
2. **Stack Overflow**: Great for specific problems
3. **GitHub**: Learn from other people's code
4. **Kaggle**: Practice with real datasets and competitions

---

## Troubleshooting

### Common Python Issues

**Problem**: `python` command not found
```bash
# Solution: Use python3 on macOS/Linux, or add Python to PATH on Windows
python3 --version  # macOS/Linux
python --version   # Windows (after adding to PATH)
```

**Problem**: Package installation fails
```bash
# Solution: Upgrade pip/uv first
pip install --upgrade pip
# or
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**Problem**: Import errors
```bash
# Solution: Check if package is installed in correct environment
uv add package_name  # Add missing package
uv sync              # Sync all dependencies
```

### Common Git Issues

**Problem**: Git not recognizing changes
```bash
# Solution: Check Git status and add files
git status
git add .
git commit -m "Your message"
```

**Problem**: Merge conflicts
```bash
# Solution: Open files, resolve conflicts manually, then:
git add .
git commit -m "Resolve merge conflict"
```

### Common VS Code Issues

**Problem**: Python interpreter not found
1. Open Command Palette (Ctrl+Shift+P)
2. Type "Python: Select Interpreter"
3. Choose your Python installation

**Problem**: Extensions not working
1. Restart VS Code
2. Check if extensions are enabled
3. Update extensions

### Common Jupyter Issues

**Problem**: Kernel not starting
```bash
# Solution: Reinstall Jupyter
uv remove jupyter
uv add jupyter
```

**Problem**: Can't see notebooks in browser
```bash
# Solution: Check if Jupyter is running on correct port
jupyter notebook --port=8888
```

### Getting Help

1. **Read Error Messages**: They usually tell you what's wrong
2. **Search Online**: Someone likely had the same problem
3. **Check Documentation**: Official docs are usually comprehensive
4. **Ask for Help**: Don't be afraid to ask classmates or instructors

---

## Google Colab: Cloud-Based Machine Learning Environment

Google Colab (Colaboratory) is a free cloud-based Jupyter notebook environment that provides powerful computing resources without requiring local installation. It's perfect for students who want to experiment with machine learning without setting up a local development environment.

### What is Google Colab?

Google Colab is a cloud-based platform that provides:
- **Free Jupyter notebooks** running in your browser
- **Free GPU and TPU access** for machine learning projects
- **Pre-installed ML libraries** (TensorFlow, PyTorch, scikit-learn, etc.)
- **Easy sharing and collaboration** features
- **Integration with Google Drive** for file storage
- **No installation required** - works in any web browser

### Why Use Google Colab?

**Advantages:**
- **No Setup Required**: Start coding immediately in your browser
- **Free Computing Resources**: Access to GPUs and TPUs for free
- **Pre-installed Libraries**: All major ML libraries are ready to use
- **Easy Sharing**: Share notebooks with a simple link
- **Version Control**: Automatic saving and version history
- **Collaboration**: Multiple people can work on the same notebook
- **Mobile Friendly**: Works on tablets and phones

**When to Use Colab vs Local Environment:**
- **Use Colab**: Learning, prototyping, sharing demos, GPU-intensive tasks
- **Use Local**: Production code, sensitive data, offline work, custom environments

### Getting Started with Google Colab

#### Step 1: Access Google Colab

1. **Visit**: [colab.research.google.com](https://colab.research.google.com)
2. **Sign in** with your Google account
3. **Click "New Notebook"** to create your first notebook

#### Step 2: Understanding the Colab Interface

**Key Components:**
- **Menu Bar**: File, Edit, View, Insert, Runtime, Tools, Help
- **Toolbar**: Code execution, cell types, sharing options
- **Notebook Area**: Where you write and run code
- **Sidebar**: Table of contents, variables, files

**Cell Types:**
- **Code Cells**: Execute Python code
- **Text Cells**: Markdown for documentation
- **Raw Cells**: Plain text (rarely used)

### Essential Colab Features

#### 1. Hardware Acceleration (GPU/TPU)

**Enable GPU:**
1. **Runtime** ‚Üí **Change runtime type**
2. **Hardware accelerator** ‚Üí **GPU** (or TPU for advanced users)
3. **Click "Save"**

**Check GPU Availability:**
```python
# Check if GPU is available
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"GPU count: {torch.cuda.device_count()}")

# For TensorFlow
import tensorflow as tf
print(f"GPU available: {tf.config.list_physical_devices('GPU')}")
```

#### 2. File Management

**Upload Files:**
```python
# Upload files from your computer
from google.colab import files
uploaded = files.upload()

# Access uploaded files
for filename in uploaded.keys():
    print(f"Uploaded: {filename}")
```

**Mount Google Drive:**
```python
# Mount Google Drive for persistent storage
from google.colab import drive
drive.mount('/content/drive')

# Access files in Drive
import os
os.listdir('/content/drive/MyDrive')
```

**Download Files:**
```python
# Download files to your computer
from google.colab import files
files.download('filename.csv')
```

#### 3. Installing Additional Packages

```python
# Install packages not pre-installed
!pip install package_name

# Install from requirements file
!pip install -r requirements.txt

# Install specific versions
!pip install pandas==1.5.0 numpy==1.24.0
```

#### 4. Magic Commands

```python
# Shell commands
!ls -la
!pwd
!python --version

# System information
%system
%env

# Time execution
%time print("Hello World")
%timeit sum(range(1000))

# Load external scripts
%load https://example.com/script.py
```

### Creating Your First Colab Notebook

#### Step 1: Basic Setup

**Cell 1 (Markdown):**
```markdown
# My First Google Colab Notebook

This notebook demonstrates basic machine learning concepts using Google Colab.

## Objectives
- Learn Colab basics
- Load and explore data
- Create visualizations
- Train a simple ML model
```

**Cell 2 (Code):**
```python
# Import essential libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

print("‚úÖ All libraries imported successfully!")
print(f"Pandas version: {pd.__version__}")
print(f"NumPy version: {np.__version__}")
```

#### Step 2: Data Loading and Exploration

**Cell 3 (Code):**
```python
# Create sample dataset
np.random.seed(42)
n_samples = 1000

# Generate features
size = np.random.normal(1500, 500, n_samples)
bedrooms = np.random.poisson(2.5, n_samples) + 1
age = np.random.exponential(10, n_samples)

# Generate target (price)
price = (size * 150 + bedrooms * 5000 + (50 - age) * 1000 + 
         np.random.normal(0, 20000, n_samples))

# Create DataFrame
data = pd.DataFrame({
    'size_sqft': size.astype(int),
    'bedrooms': bedrooms.astype(int),
    'age_years': age.round(1),
    'price': price.astype(int)
})

print("üìä Dataset created!")
print(f"Shape: {data.shape}")
data.head()
```

**Cell 4 (Code):**
```python
# Basic data exploration
print("üìà Dataset Statistics:")
print(data.describe())

print("\nüîç Missing Values:")
print(data.isnull().sum())

print("\nüìä Data Types:")
print(data.dtypes)
```

#### Step 3: Data Visualization

**Cell 5 (Code):**
```python
# Create comprehensive visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Distribution of prices
axes[0, 0].hist(data['price'], bins=30, alpha=0.7, color='skyblue')
axes[0, 0].set_title('Distribution of House Prices')
axes[0, 0].set_xlabel('Price ($)')
axes[0, 0].set_ylabel('Frequency')

# Price vs Size
axes[0, 1].scatter(data['size_sqft'], data['price'], alpha=0.6, color='green')
axes[0, 1].set_title('Price vs House Size')
axes[0, 1].set_xlabel('Size (sqft)')
axes[0, 1].set_ylabel('Price ($)')

# Price vs Bedrooms
data.boxplot(column='price', by='bedrooms', ax=axes[1, 0])
axes[1, 0].set_title('Price by Number of Bedrooms')
axes[1, 0].set_xlabel('Bedrooms')
axes[1, 0].set_ylabel('Price ($)')

# Price vs Age
axes[1, 1].scatter(data['age_years'], data['price'], alpha=0.6, color='red')
axes[1, 1].set_title('Price vs House Age')
axes[1, 1].set_xlabel('Age (years)')
axes[1, 1].set_ylabel('Price ($)')

plt.tight_layout()
plt.show()
```

**Cell 6 (Code):**
```python
# Correlation heatmap
plt.figure(figsize=(10, 8))
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
plt.show()
```

#### Step 4: Machine Learning Model

**Cell 7 (Code):**
```python
# Prepare data for machine learning
X = data[['size_sqft', 'bedrooms', 'age_years']]
y = data['price']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")
```

**Cell 8 (Code):**
```python
# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("üéØ Model Performance:")
print(f"Root Mean Square Error: ${rmse:,.2f}")
print(f"R¬≤ Score: {r2:.4f}")
print(f"Model explains {r2*100:.2f}% of the variance")
```

**Cell 9 (Code):**
```python
# Visualize predictions vs actual values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.6, color='purple')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Price ($)')
plt.ylabel('Predicted Price ($)')
plt.title('Actual vs Predicted House Prices')
plt.grid(True, alpha=0.3)
plt.show()

# Show feature importance
feature_names = ['Size (sqft)', 'Bedrooms', 'Age (years)']
coefficients = model.coef_

plt.figure(figsize=(10, 6))
plt.bar(feature_names, coefficients, color=['skyblue', 'lightgreen', 'lightcoral'])
plt.title('Feature Importance (Coefficients)')
plt.ylabel('Coefficient Value')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.show()
```

### Advanced Colab Features

#### 1. Working with External Data

**Loading from URLs:**
```python
# Load data directly from URLs
url = "https://raw.githubusercontent.com/datasets/house-prices/master/data/house-prices.csv"
data = pd.read_csv(url)
print(f"Loaded {len(data)} rows from URL")
```

**Loading from GitHub:**
```python
# Load data from GitHub repositories
github_url = "https://raw.githubusercontent.com/username/repo/main/data.csv"
data = pd.read_csv(github_url)
```

#### 2. Interactive Widgets

```python
# Install and import widgets
!pip install ipywidgets
import ipywidgets as widgets
from IPython.display import display

# Create interactive sliders
def plot_data(n_samples=100):
    x = np.random.randn(n_samples)
    y = 2 * x + np.random.randn(n_samples)
    
    plt.figure(figsize=(8, 6))
    plt.scatter(x, y, alpha=0.6)
    plt.title(f'Scatter Plot with {n_samples} samples')
    plt.show()

# Create interactive widget
widgets.interact(plot_data, n_samples=(10, 1000, 10))
```

#### 3. Forms for Input

```python
# Create forms for user input
from google.colab import widgets

# Text input form
@widgets.interact
def analyze_house(size=(500, 5000, 100), bedrooms=(1, 6, 1), age=(0, 100, 1)):
    # Predict house price using our model
    prediction = model.predict([[size, bedrooms, age]])[0]
    print(f"üè† Predicted Price: ${prediction:,.2f}")
    print(f"üìè Size: {size} sqft")
    print(f"üõèÔ∏è Bedrooms: {bedrooms}")
    print(f"üìÖ Age: {age} years")
```

#### 4. Saving and Loading Models

```python
# Save trained model
import joblib
joblib.dump(model, 'house_price_model.pkl')

# Download model file
from google.colab import files
files.download('house_price_model.pkl')

# Load model (in a new session)
# model = joblib.load('house_price_model.pkl')
```

### Colab Best Practices

#### 1. Notebook Organization

**Structure Your Notebook:**
```markdown
# Project Title

## 1. Introduction
- Project overview
- Objectives
- Data description

## 2. Data Loading and Exploration
- Load data
- Basic statistics
- Visualizations

## 3. Data Preprocessing
- Handle missing values
- Feature engineering
- Data splitting

## 4. Model Training
- Train models
- Hyperparameter tuning
- Cross-validation

## 5. Model Evaluation
- Performance metrics
- Visualizations
- Interpretation

## 6. Conclusions
- Key findings
- Limitations
- Future work
```

#### 2. Code Organization

**Use Functions:**
```python
def load_and_explore_data(url):
    """Load data from URL and perform basic exploration"""
    data = pd.read_csv(url)
    print(f"Dataset shape: {data.shape}")
    print(f"Missing values:\n{data.isnull().sum()}")
    return data

def create_visualizations(data):
    """Create comprehensive visualizations"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    # ... visualization code ...
    plt.show()

# Use the functions
data = load_and_explore_data("https://example.com/data.csv")
create_visualizations(data)
```

#### 3. Performance Optimization

**Use GPU Effectively:**
```python
# Check GPU memory
!nvidia-smi

# Clear GPU memory if needed
import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()
```

**Optimize Data Loading:**
```python
# Use efficient data types
data['price'] = data['price'].astype('float32')
data['size_sqft'] = data['size_sqft'].astype('int16')

# Use chunking for large datasets
chunk_size = 10000
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    process_chunk(chunk)
```

### Sharing and Collaboration

#### 1. Sharing Notebooks

**Public Sharing:**
1. **File** ‚Üí **Share**
2. **Change to "Anyone with the link"**
3. **Copy the link** and share

**Private Sharing:**
1. **File** ‚Üí **Share**
2. **Add specific email addresses**
3. **Set permissions** (Viewer, Commenter, Editor)

#### 2. Version Control

**Save to GitHub:**
1. **File** ‚Üí **Save a copy in GitHub**
2. **Select repository**
3. **Add commit message**
4. **Click "OK"**

**Download as Different Formats:**
```python
# Download as different formats
!jupyter nbconvert --to html notebook.ipynb
!jupyter nbconvert --to pdf notebook.ipynb
!jupyter nbconvert --to python notebook.ipynb
```

#### 3. Collaboration Features

**Comments and Suggestions:**
- **Add comments** to specific cells
- **Suggest edits** for code improvements
- **Resolve comments** when addressed

**Real-time Collaboration:**
- **Multiple users** can edit simultaneously
- **See cursors** of other collaborators
- **Automatic saving** of changes

### Colab Limitations and Workarounds

#### 1. Session Limitations

**Session Timeout:**
- **Free accounts**: 12-hour session limit
- **Solution**: Save work frequently, use Google Drive for persistence

**Resource Limits:**
- **Free GPU**: Limited usage per day
- **Solution**: Use CPU for development, GPU for final training

#### 2. Data Storage

**Temporary Storage:**
- **Files deleted** when session ends
- **Solution**: Save important files to Google Drive

**Large File Handling:**
```python
# For large files, use streaming
def process_large_file(url):
    for chunk in pd.read_csv(url, chunksize=1000):
        yield process_chunk(chunk)

# Or use cloud storage
from google.colab import drive
drive.mount('/content/drive')
```

#### 3. Custom Environment Setup

**Install Custom Packages:**
```python
# Install packages at the beginning of notebook
!pip install custom-package

# Or use requirements file
!pip install -r requirements.txt
```

**Set Up Environment Variables:**
```python
import os
os.environ['CUSTOM_VAR'] = 'value'
```

### Colab vs Local Development

#### When to Use Colab

**Choose Colab When:**
- Learning and experimenting
- Need GPU/TPU access
- Sharing demos and tutorials
- Working with large datasets
- Collaborating with others
- Quick prototyping

#### When to Use Local Environment

**Choose Local When:**
- Working with sensitive data
- Need custom environments
- Offline work required
- Production code development
- Complex project structures
- Need specific package versions

### Integration with Other Tools

#### 1. Google Drive Integration

```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Work with Drive files
import os
os.chdir('/content/drive/MyDrive/MyProject')

# Save outputs to Drive
results.to_csv('/content/drive/MyDrive/results.csv')
```

#### 2. GitHub Integration

```python
# Clone repositories
!git clone https://github.com/username/repo.git

# Push changes
!cd repo && git add . && git commit -m "Update from Colab" && git push
```

#### 3. External APIs

```python
# Make API calls
import requests

response = requests.get('https://api.example.com/data')
data = response.json()
```

### Troubleshooting Common Colab Issues

#### 1. Runtime Issues

**Problem**: Runtime disconnected
```python
# Solution: Reconnect and restart runtime
# Runtime ‚Üí Restart runtime
# Or use: Runtime ‚Üí Restart and run all
```

**Problem**: Out of memory
```python
# Solution: Clear variables and restart
%reset -f  # Clear all variables
# Or restart runtime
```

#### 2. Import Issues

**Problem**: Module not found
```python
# Solution: Install missing packages
!pip install missing-package

# Or check if package is available
import sys
print(sys.path)
```

#### 3. File Access Issues

**Problem**: File not found
```python
# Solution: Check current directory
!pwd
!ls -la

# Navigate to correct directory
import os
os.chdir('/content/drive/MyDrive/MyProject')
```

### Advanced Colab Techniques

#### 1. Custom HTML and CSS

```python
# Add custom styling
from IPython.display import HTML, display

HTML("""
<style>
.custom-header {
    background-color: #4285f4;
    color: white;
    padding: 10px;
    border-radius: 5px;
    text-align: center;
}
</style>
<div class="custom-header">
    <h2>Custom Styled Header</h2>
</div>
""")
```

#### 2. Interactive Plots

```python
# Create interactive plots with Plotly
!pip install plotly

import plotly.express as px
import plotly.graph_objects as go

# Interactive scatter plot
fig = px.scatter(data, x='size_sqft', y='price', 
                 color='bedrooms', size='age_years',
                 hover_data=['price'])
fig.show()
```

#### 3. Custom Functions and Classes

```python
class HousePricePredictor:
    def __init__(self):
        self.model = None
        self.feature_names = ['size_sqft', 'bedrooms', 'age_years']
    
    def train(self, X, y):
        self.model = LinearRegression()
        self.model.fit(X, y)
        return self
    
    def predict(self, X):
        return self.model.predict(X)
    
    def get_feature_importance(self):
        return dict(zip(self.feature_names, self.model.coef_))

# Use the class
predictor = HousePricePredictor()
predictor.train(X_train, y_train)
predictions = predictor.predict(X_test)
```

### Colab Security Best Practices

#### 1. Data Privacy

**Sensitive Data:**
- **Never upload** sensitive personal data
- **Use synthetic data** for demonstrations
- **Anonymize data** before uploading

**API Keys:**
```python
# Store API keys securely
import os
api_key = os.environ.get('API_KEY')

# Or use Colab secrets
from google.colab import userdata
api_key = userdata.get('API_KEY')
```

#### 2. Code Security

**Review Shared Code:**
- **Check permissions** before sharing
- **Remove sensitive information** from notebooks
- **Use private sharing** for sensitive projects

### Conclusion

Google Colab is an excellent platform for learning machine learning and data science. It provides:

- **Easy access** to powerful computing resources
- **No setup required** - start coding immediately
- **Free GPU access** for training models
- **Excellent collaboration** features
- **Integration** with Google services

**Next Steps:**
1. **Practice** with the examples in this tutorial
2. **Explore** different datasets and models
3. **Share** your notebooks with others
4. **Learn** advanced features as you progress
5. **Combine** Colab with local development as needed

Remember: Colab is a great tool for learning and prototyping, but for production work, consider setting up a local development environment as described in the earlier sections of this tutorial.

---

## Conclusion

Congratulations! üéâ You've successfully set up a complete machine learning development environment. You now have:

- ‚úÖ Python installed and configured
- ‚úÖ Git for version control
- ‚úÖ uv for fast package management
- ‚úÖ VS Code as your IDE
- ‚úÖ Jupyter notebooks for interactive analysis
- ‚úÖ A complete project template
- ‚úÖ Best practices for organizing your work

### Next Steps

1. **Practice**: Create more projects and experiment
2. **Learn**: Explore machine learning libraries (scikit-learn, pandas, matplotlib)
3. **Build**: Start working on real problems that interest you
4. **Share**: Use Git and GitHub to share your work
5. **Collaborate**: Work with others on projects

### Remember

- **Start Small**: Begin with simple projects and gradually increase complexity
- **Be Patient**: Learning takes time, and everyone makes mistakes
- **Stay Curious**: The field of machine learning is constantly evolving
- **Have Fun**: Enjoy the journey of discovery and learning!

Welcome to the exciting world of Machine Learning! üöÄ

---

*This tutorial was created for first-year students beginning their journey in machine learning and data science. For questions or suggestions, please refer to your course materials or ask your instructors.*
